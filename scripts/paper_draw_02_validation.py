# -*- coding: utf-8 -*-
"""
scripts/paper_draw_02_validation.py

[PAEC Framework - Phase 4: Validation Analysis & Visualization]

This script generates the key figures for Chapter 5 of the PAEC paper, focusing on the
comparative analysis between PAEC kNN-MT and baselines (Adaptive kNN-MT, Vanilla, Pure NMT).

It analyzes the results generated by `paec_mt_validation.py` to produce:
1. Macro-Dynamics: Rank distribution of models in high/low-risk scenarios.
2. Micro-Dynamics: Step-by-step trajectory visualization of specific "Achilles' Heel"
   samples (e.g., Sample 762), highlighting where PAEC prevents divergence.
3. Trends: Cumulative win rates sorted by initial risk.
4. Statistics: Formal significance tests (T-test, McNemar's Test).

Output Images:
- rank_dist_v_high.png: Stability ranking in high-risk samples.
- rank_dist_v_low.png: Stability ranking in low-risk samples.
- win_rate_trend_high_risk.png: PAEC's advantage as risk increases.
- micro_dynamics_max_separation.png: Trajectories of top divergent samples.
- trajectory_achilles_sample_762.png: Specific case study visualization.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os, sys, traceback
from sklearn.cluster import KMeans
from scipy import stats
from pathlib import Path

# --- Configuration & Path Setup ---
# Dynamically locate the project root to import configuration modules.
try:
    project_root = Path(__file__).parent.parent.resolve()
    sys.path.append(str(project_root))
    from src.config import PATHS

except ImportError:
    print("Error: Could not import PATHS from src.config.")
    print("  Ensure 'src/config.py' exists and defines PATHS.")
    traceback.print_exc()
    sys.exit(1)
except Exception as e:
    print(f"Error: An unexpected error occurred during config import: {e}")
    traceback.print_exc()
    sys.exit(1)

# Define directories for input data (from validation runs) and output images.
BASE_DIR = PATHS["results_dir"] / r'paec_validation'
IMAGES_DIR = os.path.join(BASE_DIR, 'images')
STEP_FILE = os.path.join(BASE_DIR, 'detailed_step_results.csv')
FINAL_FILE = os.path.join(BASE_DIR, 'detailed_final_results.csv')

# Define standardized model display names matching the paper's terminology.
M_ADAPTIVE = 'Adaptive-kNN-MT'
M_PAEC_OFF = 'PAEC kNN-MT (Offline π)'
M_PAEC_ON  = 'PAEC kNN-MT (Online)'
M_PURE     = 'Vanilla Pure NMT'
M_VANILLA  = 'Vanilla-kNN-MT'

# Ensure the directory for saving plots exists.
os.makedirs(IMAGES_DIR, exist_ok=True)

# --- Helper Functions for Data Processing & Visualization ---

def get_rank_counts(pivot_df, sample_ids, ascending=True):
    """
    Calculates the distribution of performance ranks (1st to 5th) for each model
    within a specific subset of samples (e.g., high-risk group).

    Args:
        pivot_df: DataFrame with sample_ids as index and models as columns.
        sample_ids: List of sample IDs to include in the analysis.
        ascending: If True, lower values (e.g., V_error) get better ranks (1).

    Returns:
        DataFrame: Percentages of achieving each rank per model.
    """
    # Filter the data for the specified sample IDs.
    valid_ids = pivot_df.index.intersection(sample_ids)
    if len(valid_ids) == 0: return None

    subset = pivot_df.loc[valid_ids]

    # Compute ranks for each sample across models.
    # method='min' assigns the same rank to ties (e.g., if two are best, both are Rank 1).
    ranks = subset.rank(axis=1, method='min', ascending=ascending)

    # Count how often each model achieved each rank (1.0, 2.0, etc.).
    counts = ranks.apply(lambda x: x.value_counts()).fillna(0)

    # Convert counts to percentages.
    percents = counts.div(counts.sum(axis=0), axis=1) * 100

    return percents.T

def plot_stacked_ranks(rank_df, title, filename):
    """
    Generates a 100% stacked bar chart displaying the rank distribution for each model.
    Highlights the percentage of "Rank 1" (Best) achievements.

    Args:
        rank_df: DataFrame containing rank percentages (output of get_rank_counts).
        title: Title of the plot.
        filename: Output filename.
    """
    if rank_df is None or rank_df.empty:
        print(f"[Warning] No data to plot for {filename}")
        return

    # Ensure all rank columns (1.0 to 5.0) exist, filling zeros if missing.
    for r in [1.0, 2.0, 3.0, 4.0, 5.0]:
        if r not in rank_df.columns:
            rank_df[r] = 0.0
    rank_df = rank_df[[1.0, 2.0, 3.0, 4.0, 5.0]]

    # Define the specific order of models for the x-axis.
    model_order = [M_PURE, M_VANILLA, M_ADAPTIVE, M_PAEC_ON, M_PAEC_OFF]

    # Filter to keep only models present in the current dataset.
    existing_models = [m for m in model_order if m in rank_df.index]
    data = rank_df.loc[existing_models]

    # Setup plot aesthetics.
    fig, ax = plt.subplots(figsize=(10, 6))
    # Color palette: Gold (1st), Silver (2nd), Bronze (3rd), Gray, Light Gray.
    colors = ['#FFD700', '#C0C0C0', '#CD7F32', '#808080', '#D3D3D3']

    # Create the stacked bar chart.
    data.plot(kind='bar', stacked=True, ax=ax, color=colors, edgecolor='black', width=0.7, alpha=0.9)

    # Annotate the "Rank 1" segments with their percentage values.
    for c in ax.containers:
        if c.get_label() == '1.0':
            ax.bar_label(c, fmt='%.1f%%', label_type='center', color='black', weight='bold', fontsize=9)

    ax.set_title(title, fontsize=14, pad=15, fontweight='bold')
    ax.set_ylabel('Percentage of Samples (%)', fontsize=12)
    ax.set_xlabel('', fontsize=12)
    ax.set_ylim(0, 100)
    plt.xticks(rotation=15, ha='right', fontsize=11)

    # Configure legend.
    labels = [f"Rank {int(r)} (Best)" if r==1 else f"Rank {int(r)}" for r in [1,2,3,4,5]]
    ax.legend(labels, title='Performance Rank', bbox_to_anchor=(1.02, 1), loc='upper left')

    plt.tight_layout()
    save_path = os.path.join(IMAGES_DIR, filename)
    plt.savefig(save_path, dpi=300)
    print(f"[Info] Saved Rank Plot: {save_path}")
    plt.close(fig)

def plot_trajectory_with_fill(ax, sample_data, sample_id, show_ylabel=False):
    """
    Visualizes the Lyapunov energy trajectory (V_error vs Step) for a single sample.
    Highlights the "Stabilization Gain" (area where PAEC < Adaptive) using shading.

    Args:
        ax: Matplotlib axes object.
        sample_data: DataFrame containing step-wise data for the specific sample.
        sample_id: ID of the sample being plotted.
        show_ylabel: Boolean to toggle y-axis label visibility.

    Returns:
        List of plot handles for legend creation.
    """
    # Extract trajectory data for the key models.
    df_adapt = sample_data[sample_data['model_name'] == M_ADAPTIVE].sort_values('step')
    df_paec  = sample_data[sample_data['model_name'] == M_PAEC_OFF].sort_values('step')
    df_pure  = sample_data[sample_data['model_name'] == M_PURE].sort_values('step')

    # Align trajectory lengths (truncate to the shorter one for comparison).
    min_len = min(len(df_adapt), len(df_paec))

    if min_len == 0: return []

    df_adapt = df_adapt.iloc[:min_len]
    df_paec = df_paec.iloc[:min_len]
    x_axis = df_adapt['step']

    # 1. Plot Pure NMT (Baseline) as a faint background reference.
    if not df_pure.empty:
        ax.plot(df_pure['step'], df_pure['v_error'], color='gray', alpha=0.3, linewidth=1, linestyle=':')

    # 2. Plot Adaptive kNN-MT (Red, Dashed) to represent baseline divergence risk.
    line_adapt, = ax.plot(x_axis, df_adapt['v_error'],
                         color='#d62728', linestyle='--', linewidth=2,
                         label='Adaptive (Baseline)')

    # 3. Plot PAEC Offline (Green, Solid) to represent controlled stability.
    line_paec, = ax.plot(x_axis, df_paec['v_error'],
                        color='#2ca02c', linestyle='-', linewidth=2.5,
                        label='PAEC (Offline)')

    # 4. Shade the area where PAEC outperforms Adaptive (Stabilization Gain).
    y1 = df_adapt['v_error'].values
    y2 = df_paec['v_error'].values
    ax.fill_between(x_axis, y1, y2, where=(y1 > y2),
                   interpolate=True, color='#2ca02c', alpha=0.15)

    # 5. Annotate the final reduction in Lyapunov energy (Delta V).
    end_step = x_axis.iloc[-1]
    end_v_adapt = y1[-1]
    end_v_paec = y2[-1]

    if abs(end_v_adapt - end_v_paec) > 0.1:
        ax.annotate(f'ΔV={end_v_adapt - end_v_paec:.2f}',
                   xy=(end_step, (end_v_adapt + end_v_paec)/2),
                   xytext=(10, 0), textcoords='offset points',
                   arrowprops=dict(arrowstyle='-|>', color='black', lw=0.8),
                   fontsize=9, va='center', fontweight='bold')

    # Configure axes and styling.
    ax.set_title(f'Sample {sample_id}', fontsize=11, pad=5)
    ax.set_xlabel('Decoding Step', fontsize=10)
    if show_ylabel:
        ax.set_ylabel(r'Error Energy $V(\mathcal{E}_t)$', fontsize=10)

    # Add a "Danger Zone" background (V > 0.8) to indicate instability.
    max_v = max(df_adapt['v_error'].max(), df_paec['v_error'].max())
    ax.axhspan(0.8, max(1.5, max_v + 0.1), color='red', alpha=0.03, zorder=0)

    return [line_adapt, line_paec]

# --- Statistical Analysis ---

def perform_statistical_tests(pivot_df, model_a, model_b, risk_ids=None):
    """
    Performs statistical significance tests to compare two models.
    1. Paired T-test: Compares the continuous distributions of V(E).
    2. McNemar's Test: Compares binary failure rates (V > 0.5 threshold).

    Args:
        pivot_df: DataFrame with sample IDs as index and model names as columns (values are V_error).
        model_a: Name of the first model.
        model_b: Name of the second model (baseline).
        risk_ids: Optional list of sample IDs to restrict the scope (e.g., High Risk only).
    """
    print(f"\n[STATISTICS] Comparing {model_a} vs {model_b}...")

    subset = pivot_df

    if risk_ids is not None:
        subset = pivot_df.loc[pivot_df.index.intersection(risk_ids)]
        print(f"  > Scope: {len(subset)} samples (Subset)")
    else:
        print(f"  > Scope: {len(subset)} samples (Full Set)")

    # Prepare paired data series.
    vec_a = subset[model_a].dropna()
    vec_b = subset[model_b].dropna()

    # Align indices to ensure valid pairing.
    common_idx = vec_a.index.intersection(vec_b.index)
    vec_a = vec_a.loc[common_idx]
    vec_b = vec_b.loc[common_idx]

    # 1. Paired T-test on Lyapunov Error V(E).
    t_stat, p_val_t = stats.ttest_rel(vec_a, vec_b)
    print(f"  > Paired t-test on V(E): t={t_stat:.4f}, p={p_val_t:.4e}")
    if p_val_t < 0.05: print("    -> Difference is STATISTICALLY SIGNIFICANT.")

    # 2. McNemar's Test on Failure Rate (V > 0.5).
    # Construct Contingency Table:
    #             Model B (Pass) | Model B (Fail)
    # Model A (Pass)     d       |      c
    # Model A (Fail)     b       |      a
    threshold = 0.5
    a_fail = (vec_a > threshold)
    b_fail = (vec_b > threshold)

    # b: A fails, B passes (A is worse)
    # c: A passes, B fails (A is better)
    b_count = ((a_fail) & (~b_fail)).sum()
    c_count = ((~a_fail) & (b_fail)).sum()

    # Calculate McNemar statistic and p-value.
    if (b_count + c_count) > 0:
        res = stats.binomtest(b_count, b_count + c_count, 0.5)
        p_val_mcnemar = res.pvalue
        print(f"  > McNemar's Test on Failure Rate: discordants (A_fail/B_pass={b_count}, A_pass/B_fail={c_count}), p={p_val_mcnemar:.4e}")
    else:
        print("  > McNemar's Test: No discordant pairs.")

# --- Main Execution Flow ---

def main():
    print("=== Starting PAEC Final Figure Generation ===")

    # 1. Load Data
    try:
        df_steps = pd.read_csv(STEP_FILE)
        print(f"[Info] Loaded step data: {len(df_steps)} rows")
    except FileNotFoundError:
        print(f"[Error] File not found: {STEP_FILE}")
        return

    # 2. Define Risk Groups based on initial state (Step 0) snapshot.
    print("[Info] Defining Risk Groups...")
    step0 = df_steps[df_steps['step'] == 0]

    # Use PAEC Online's initial assessment if available, otherwise fallback to mean.
    if M_PAEC_ON in step0['model_name'].values:
        ref_step0 = step0[step0['model_name'] == M_PAEC_ON].set_index('sample_id')
    else:
        # Fallback: Average across models if specific one is missing.
        ref_step0 = step0.groupby('sample_id').mean(numeric_only=True)

    # Define High vs Low risk based on error coverage and Lyapunov energy V(E).
    # Low Risk: Perfect coverage and low initial V.
    low_risk_ids = ref_step0[
        (ref_step0['error_coverage'] == 0.0) & (ref_step0['v_error'] < 0.8)
    ].index

    # High Risk: Missing entities OR high initial V.
    high_risk_ids = ref_step0[
        (ref_step0['error_coverage'] == 1.0) | (ref_step0['v_error'] > 1.2)
    ].index

    print(f"  > High-Risk Samples: {len(high_risk_ids)}")
    print(f"  > Low-Risk Samples: {len(low_risk_ids)}")

    # 3. Process Final Results for Ranking and Analysis.
    # Extract the final step for each sample/model to get the convergence state.
    idx = df_steps.groupby(['model_name', 'sample_id'])['step'].idxmax()
    final_v = df_steps.loc[idx, ['model_name', 'sample_id', 'v_error']].copy()
    pivot_v = final_v.pivot(index='sample_id', columns='model_name', values='v_error')

    # --- Task 1: Generate Rank Distribution Plots (Macro-Dynamics) ---
    print("\n[Info] Task 1: Generating Rank Distribution Plots...")

    # Generate rank distribution for High-Risk Samples.
    ranks_high = get_rank_counts(pivot_v, high_risk_ids, ascending=True)
    plot_stacked_ranks(
        ranks_high,
        'Stability Ranking on High-Risk Samples\n(Metric: Final V(E), Rank 1 = Lowest Error)',
        'rank_dist_v_high.png'
    )

    # Generate rank distribution for Low-Risk Samples.
    ranks_low = get_rank_counts(pivot_v, low_risk_ids, ascending=True)
    plot_stacked_ranks(
        ranks_low,
        'Stability Ranking on Low-Risk Samples\n(Metric: Final V(E), Rank 1 = Lowest Error)',
        'rank_dist_v_low.png'
    )

    # --- Task 2: Generate Cumulative Win Rate Trend ---
    print("\n[Info] Task 2: Generating Cumulative Win Rate Trend...")

    # Filter for High-Risk samples to analyze behavior under stress.
    subset = pivot_v.loc[pivot_v.index.intersection(high_risk_ids)]
    # Retrieve initial risk values for sorting.
    subset_ref = ref_step0.loc[subset.index]
    # Sort samples from Highest Initial Risk -> Lowest Initial Risk.
    sorted_indices = subset_ref.sort_values('v_error', ascending=False).index
    outcomes = subset.loc[sorted_indices]

    # Define a "Win": PAEC achieves lower V_error than Adaptive.
    win_online = (outcomes[M_PAEC_ON] < outcomes[M_ADAPTIVE]).astype(int)
    win_offline = (outcomes[M_PAEC_OFF] < outcomes[M_ADAPTIVE]).astype(int)

    # Calculate expanding mean (Cumulative Win Rate) across the sorted list.
    cum_win_online = win_online.expanding().mean() * 100
    cum_win_offline = win_offline.expanding().mean() * 100

    # Plot the Trend.
    fig, ax = plt.subplots(figsize=(12, 6))
    x = np.arange(len(cum_win_online)) + 1

    ax.plot(x, cum_win_online, label='PAEC Online vs Adaptive', color='#2ca02c', linewidth=2)
    ax.plot(x, cum_win_offline, label='PAEC Offline vs Adaptive', color='#9467bd', linewidth=2, linestyle='--')

    # Add 50% reference line.
    ax.axhline(50, color='gray', linestyle=':', alpha=0.5)

    ax.set_title('Cumulative Win Rate Trend in High-Risk Scenarios\n(Samples Sorted by Initial Risk: Highest -> Lowest)', fontsize=14)
    ax.set_xlabel('Number of Samples Included (Top-N Riskiest)', fontsize=12)
    ax.set_ylabel('Cumulative Win Rate (%)', fontsize=12)
    ax.legend(fontsize=11)
    ax.grid(True, linestyle='--', alpha=0.6)

    # Annotate risk zones.
    ax.text(0, cum_win_online.iloc[0]+1, 'Extreme Risk', fontsize=10, fontweight='bold')
    ax.text(len(x)-50, cum_win_online.iloc[-1]-2, 'Moderate Risk', fontsize=10, fontweight='bold', ha='right')

    plt.tight_layout()
    save_path_trend = os.path.join(IMAGES_DIR, 'win_rate_trend_high_risk.png')
    plt.savefig(save_path_trend, dpi=300)
    print(f"[Info] Saved Trend Plot: {save_path_trend}")
    plt.close(fig)

    # --- Task 3: Statistical Significance Reporting ---
    print("\n[Info] Task 3: Running Statistical Significance Tests...")
    # Compare PAEC Offline vs Adaptive (Main Baseline).
    perform_statistical_tests(pivot_v, M_PAEC_OFF, M_ADAPTIVE)

    # Compare PAEC Offline vs Vanilla (Secondary Baseline).
    perform_statistical_tests(pivot_v, M_PAEC_OFF, M_VANILLA)

    # --- Task 4: Generate Micro-Trajectory Plots (Achilles' Heel) ---
    print("\n[Info] Task 4: Generating Micro-Trajectory Plots...")

    # 3.1 Prepare data for clustering to identify representative samples.
    # We focus on high-risk samples where Adaptive performs poorly (V > 0.5).
    target_pool = pivot_v.loc[high_risk_ids].dropna()

    # Create feature vectors: [V_Adaptive, V_PAEC_Offline].
    X = target_pool[[M_ADAPTIVE, M_PAEC_OFF]].values

    # 3.2 Perform K-Means Clustering (k=3: Win, Loss, Draw).
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X)
    centers = kmeans.cluster_centers_

    # 3.3 Identify the "PAEC Advantage" Cluster.
    # This is the cluster where the gap (Adaptive - PAEC) is maximized positively.
    center_gaps = centers[:, 0] - centers[:, 1]
    target_cluster_idx = np.argmax(center_gaps)

    # 3.4 Select Representative Samples from the advantage cluster.
    indices_in_cluster = np.where(cluster_labels == target_cluster_idx)[0]
    samples_in_cluster = target_pool.index[indices_in_cluster]
    subset = target_pool.loc[samples_in_cluster].copy()

    subset['gap'] = subset[M_ADAPTIVE] - subset[M_PAEC_OFF]

    # Apply strict constraint: PAEC must have converged (V < 0.3) to be a valid "save".
    valid_subset = subset[subset[M_PAEC_OFF] < 0.3]

    if valid_subset.empty:
        print("[Warning] No strictly converged samples in target cluster. Relaxing...")
        valid_subset = subset  # Fallback

    # Sort by the performance gap to find maximum separation.
    top_samples = valid_subset.sort_values('gap', ascending=False).head(4).index.tolist()

    # Ensure the specific case study (Sample 762) is included for the paper.
    if 762 in high_risk_ids and 762 not in top_samples:
        print("[Info] Injecting Sample 762 for visualization.")
        top_samples[3] = 762  # Inject into the visualization set.

    print(f"  > Selected Representative Samples: {top_samples}")

    # 3.5 Generate and Save the 2x2 Grid Plot.
    sns.set_theme(style="whitegrid")
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    fig.suptitle("Micro-Dynamics of 'Achilles' Heel' Scenarios\n(Samples with Significant Stabilization Gain)",
                 fontsize=14, fontweight='bold', y=0.98)

    axes_flat = axes.flatten()
    handles = []

    for i, sample_id in enumerate(top_samples):
        sample_data = df_steps[df_steps['sample_id'] == sample_id]
        ax = axes_flat[i]
        show_y = (i % 2 == 0) # Show Y-label only on left plots
        h = plot_trajectory_with_fill(ax, sample_data, sample_id, show_ylabel=show_y)
        if i == 0: handles = h

        # Also save an individual high-res plot for Sample 762 if processed.
        if sample_id == 762:
            fig_single, ax_single = plt.subplots(figsize=(8, 5))
            plot_trajectory_with_fill(ax_single, sample_data, sample_id, show_ylabel=True)
            ax_single.legend(['Adaptive', 'PAEC Offline'], loc='upper left')
            single_path = os.path.join(IMAGES_DIR, 'trajectory_achilles_sample_762.png')
            fig_single.savefig(single_path, dpi=300, bbox_inches='tight')
            print(f"[Info] Saved Specific Plot: {single_path}")
            plt.close(fig_single)

    # Add shared legend to the grid plot.
    fig.legend(handles, ['Adaptive kNN-MT', 'PAEC Offline'],
              loc='lower center', bbox_to_anchor=(0.5, 0.02),
              ncol=2, fontsize=11, frameon=True, shadow=True)

    plt.tight_layout()
    plt.subplots_adjust(bottom=0.12, top=0.90, wspace=0.2, hspace=0.35)

    grid_path = os.path.join(IMAGES_DIR, 'micro_dynamics_max_separation.png')
    fig.savefig(grid_path, dpi=300, bbox_inches='tight')
    print(f"[Info] Saved Grid Plot: {grid_path}")
    plt.close(fig)

    print("=== All Plots Generated Successfully ===")

if __name__ == "__main__":
    main()
