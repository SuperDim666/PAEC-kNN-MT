# -*- coding: utf-8 -*-
"""
src/data_processing/loader.py

[PAEC Framework - Data Loading Module]

This module defines the RealDatasetLoader class, which serves as the data ingestion layer
for the PAEC data generation and evaluation pipelines.

Key Responsibility:
- Loads source/target sentence pairs from the pre-processed "datastore subset" corpus files
  (generated by scripts/01_data_preparations/01_prepare_corpus.py).
- Ensures that the data used for generating dynamics model training data (T_theta) and
  evaluating the system is strictly consistent with the subset used to build the FAISS index.
- Applies run-time filtering (e.g., length constraints, NER filtering) based on global configuration.

This design prevents data leakage and ensures that the "Teacher" policy operates on
data it has "seen" in the Datastore context, while still testing generalization on
valid/test splits.
"""

import random, os, spacy, traceback
from typing import List, Dict, Optional
from tqdm import tqdm
import pandas as pd
from pathlib import Path

# Import project-specific configuration to access global paths and parameters
from src import config

class RealDatasetLoader:
    """
    A unified loader for accessing the normalized, local raw text corpora.
    
    It reads from the file paths defined in `config.PATHS["raw_corpus_dir"]` and
    specifically targets the *_ds.{lang} files created during the corpus preparation phase.
    """

    def __init__(self, verbose: bool = True):
        """
        Initializes the RealDatasetLoader.

        Sets up file paths, language settings, and optional filter models (like spaCy)
        based on the global `config.DATA_LOADER_PARAMS`.

        Args:
            verbose (bool): If True, prints detailed status messages during initialization.
        """
        print("[Info] Initializing Real Dataset Loader (V3 - Local Corpus Mode)...")
        
        # 1. Load parameters from the global configuration dictionary
        self.loader_params = config.DATA_LOADER_PARAMS
        self.global_ner_filter = self.loader_params.get("ner_filter", False)
        self.global_min_entities = self.loader_params.get("min_entities", 1)
        
        # 2. Define the *single source of truth* for raw data directory
        # This directory must contain the output files from '0A_prepare_corpus.py'
        self.RAW_CORPUS_DIR = config.PATHS.get("raw_corpus_dir", "")
        if not self.RAW_CORPUS_DIR or not Path(self.RAW_CORPUS_DIR).exists():
            raise FileNotFoundError(f"Raw corpus directory defined in config.PATHS['raw_corpus_dir'] not found: {self.RAW_CORPUS_DIR}")
        
        # Define the file name prefixes for different data splits and subsets.
        # "full" refers to the NMT training data, "ds" refers to the Datastore subset.
        # This loader primarily focuses on the "ds" subset.
        self.ds_split_prefixes = {
            "train": {
                "full": "train",
                "ds": "train_ds",
            },
            "validation": {
                "full": "valid",
                "ds": "valid_ds",
            },
            "test": {
                "full": "test",
                "ds": "test_ds",
            },
        }
        self.LANG_SRC = "de"
        self.LANG_TGT = "en"
        
        self.verbose = verbose
        
        # 3. Conditionally load the spaCy model for Named Entity Recognition (NER)
        # This is only required if 'ner_filter' is enabled in the config.
        self.nlp_de = None
        if self.global_ner_filter:
            try:
                self.nlp_de = spacy.load("de_core_news_sm")
                if self.verbose: print("\t- NER filter enabled. spaCy model 'de_core_news_sm' loaded.")
            except IOError:
                print("\t- [Warning] spaCy model 'de_core_news_sm' not found. NER filtering will be disabled.")
                print("\t- Please run: python -m spacy download de_core_news_sm")
                self.global_ner_filter = False
        
        if self.verbose: 
            print(f"\t- Configured to load data from: {self.RAW_CORPUS_DIR}")

    def has_entities(self, text: str, min_entities: int) -> bool:
        """
        Checks if the provided text contains a minimum number of named entities.

        Args:
            text (str): The source text to analyze.
            min_entities (int): The minimum required number of entities.

        Returns:
            bool: True if the entity count requirement is met (or if the NER model failed to load), False otherwise.
        """
        if self.nlp_de is not None:
            doc = self.nlp_de(text)
            return len(doc.ents) >= min_entities
        # If no NLP model is available (but method called), fail open to avoid dropping data unnecessarily.
        return True 

    def load_all_datasets(self, split: str = "train", size: str = "ds") -> List[Dict[str, str]]:
        """
        Loads, filters, and returns dataset samples from the specified local files.
        
        This method reads the corresponding .de and .en files line-by-line, ensuring strict
        alignment. It then applies length filtering, deduplication, and optional NER filtering.

        Args:
            split (str): The dataset split to load ('train', 'validation', or 'test').
            size (str): The subset size identifier (typically 'ds' for the datastore subset).

        Returns:
            List[Dict[str, str]]: A list of dictionaries, where each dictionary represents a sample:
                                  {'source_text': str, 'target_text': str, ...}
        
        Raises:
            ValueError: If an invalid split name is provided or file line counts mismatch.
            FileNotFoundError: If the source or target files do not exist.
        """
        all_samples = []
        if split not in self.ds_split_prefixes:
            raise ValueError(f"Invalid split name '{split}'. Must be one of {list(self.ds_split_prefixes.keys())}")
        
        # Determine the file prefix based on split and size (e.g., 'train_ds')
        split_prefix = self.ds_split_prefixes[split][size]
        if self.verbose: print(f"[Begin] Loading datastore subset '{split}' split ({split_prefix}.*)")

        # 1. Construct absolute file paths
        src_file = Path(self.RAW_CORPUS_DIR) / f"{split_prefix}.{self.LANG_SRC}"
        tgt_file = Path(self.RAW_CORPUS_DIR) / f"{split_prefix}.{self.LANG_TGT}"
        
        # Verify file existence before attempting to read
        if not (src_file.exists() and tgt_file.exists()):
            print(f"[Error] Datastore subset files for split '{split}' not found at:")
            print(f"\t1. {src_file}")
            print(f"\t2. {tgt_file}")
            print(f"\tEnsure 'scripts/01_data_preparations/01_prepare_corpus.py' created '{split_prefix}.{self.LANG_SRC}' and '{split_prefix}.{self.LANG_TGT}'.")
            raise FileNotFoundError(f"Datastore subset files for split '{split}' missing.")

        # 2. Read content from files
        print(f"\t[Loading] Reading from {src_file.name} and {tgt_file.name}...")
        try:
            with open(src_file, 'r', encoding='utf-8') as f_src:
                source_lines = f_src.readlines()
            with open(tgt_file, 'r', encoding='utf-8') as f_tgt:
                target_lines = f_tgt.readlines()
        except Exception as e:
            print(f"\t[Failure] Could not read files for split '{split}': {e}")
            traceback.print_exc()
            raise e

        # Strict alignment check: Source and Target files must have exactly the same number of lines
        if len(source_lines) != len(target_lines):
            print(f"\t[Failure] Mismatch in line count for split '{split}': {len(source_lines)} (src) vs {len(target_lines)} (tgt).")
            raise ValueError(f"Source and target files for split '{split}' have different number of lines.")

        print(f"\t[Success] Read {len(source_lines)} total pairs from '{split}' datastore subset corpus.")

        # 3. Apply filters defined in configuration
        min_sentence_length = self.loader_params.get("min_sentence_length", 5)
        max_sentence_length = self.loader_params.get("max_sentence_length", 35)
        min_entities = self.global_min_entities
        filtered_count = 0

        # Create an iterator with a progress bar for user feedback
        iterator = tqdm(
            zip(source_lines, target_lines),
            total=len(source_lines),
            desc=f"  -> Filtering '{split}' subset"
        )
        
        for de_text_raw, en_text_raw in iterator:
            de_text = de_text_raw.strip()
            tgt_text = en_text_raw.strip()

            # Filter 1: Basic Validity (Non-empty)
            if not de_text or not tgt_text:
                filtered_count += 1
                continue
            
            # Filter 2: Length Constraints
            src_len = len(de_text.split())
            tgt_len = len(tgt_text.split())
            if not (min_sentence_length <= src_len <= max_sentence_length and min_sentence_length <= tgt_len <= max_sentence_length):
                filtered_count += 1
                continue

            # Filter 3: Named Entity Recognition (Optional)
            if self.global_ner_filter:
                if not self.has_entities(de_text, min_entities):
                    filtered_count += 1
                    continue
            
            # Construct the sample object
            all_samples.append({
                'source_text': de_text,
                'target_text': tgt_text,
                'source_lang': self.LANG_SRC,
                'target_lang': self.LANG_TGT,
                'dataset': f'paec_ds_{split}', # Tag origin for traceability
                'domain': 'mixed'              # Domain metadata (simplified here)
            })

        print(f"\t[Success] Kept {len(all_samples)} pairs after filtering (removed {filtered_count}).")

        if self.verbose:
            print(f"[Complete] Loaded {len(all_samples)} samples from the '{split}' datastore subset.")

        return all_samples
